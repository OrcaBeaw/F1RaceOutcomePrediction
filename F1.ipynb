{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "%pip install requests pandas \n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'F1dataset'\n",
      "/home/penguin/Documents/Code/F1RaceOutcomePrediction/F1 dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#get data \u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m circuits \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcircuits.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m drivers \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrivers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m lap_times \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlap_times.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#get data \n",
    "%cd F1dataset \n",
    "circuits = pd.read_csv('circuits.csv')\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "lap_times = pd.read_csv('lap_times.csv')\n",
    "pit_stops = pd.read_csv('pit_stops.csv') \n",
    "qualifying = pd.read_csv('qualifying.csv')\n",
    "results = pd.read_csv('results.csv')\n",
    "seasons = pd.read_csv('seasons.csv')\n",
    "status = pd.read_csv('status.csv')\n",
    "\n",
    "#Explore the datasets\n",
    "datasets = {\n",
    "    \"Circuits\": circuits,\n",
    "    \"Drivers\": drivers,\n",
    "    \"Lap Times\": lap_times,\n",
    "    \"Pit Stops\": pit_stops,\n",
    "    \"Qualifying\": qualifying,\n",
    "    \"Results\": results,\n",
    "    \"Seasons\": seasons,\n",
    "    \"Status\": status\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "lap_times = datasets['Lap Times']\n",
    "drivers = datasets['Drivers']\n",
    "circuits = datasets['Circuits']\n",
    "\n",
    "\n",
    "#Data merging\n",
    "merged_data = lap_times.merge(drivers, on='driverId', how='inner')\n",
    "merged_data = merged_data.merge(circuits, left_on='driverId', right_on='circuitId', how='inner')\n",
    "\n",
    "#Convert lap times to seconds\n",
    "import pandas as pd\n",
    "\n",
    "def convert_to_seconds(time_str):\n",
    "    if isinstance(time_str, str):\n",
    "        minutes, seconds = time_str.split(':')\n",
    "        return int(minutes) * 60 + float(seconds)\n",
    "    return time_str\n",
    "\n",
    "# Ensure the correct column name is used\n",
    "if 'milliseconds' in merged_data.columns:\n",
    "    merged_data['lapTime'] = merged_data['milliseconds'].apply(lambda x: x / 1000)\n",
    "else:\n",
    "    raise KeyError(\"Column 'milliseconds' not found in the merged dataframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all lap times are converted to seconds before transforming\n",
    "merged_data['lapTime'] = merged_data['lapTime'].apply(convert_to_seconds)\n",
    "merged_data = merged_data.dropna(subset=['lapTime'])\n",
    "feature_columns = ['driverId', 'circuitId', 'lapTime']  \n",
    "\n",
    "# Separate features and target variable\n",
    "X = merged_data[feature_columns]\n",
    "y = merged_data['lapTime']  # If you're predicting future lap times, adjust accordingly\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['driverId', 'circuitId']\n",
    "numerical_features = ['lapTime']  # Include other numerical features if any\n",
    "\n",
    "# Transformer and scaler\n",
    "one_hot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot', one_hot, categorical_features),\n",
    "        ('scaler', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Exclude other columns not specified\n",
    ")\n",
    "\n",
    "# Apply the transformer to features\n",
    "X_transformed = transformer.fit_transform(X)\n",
    "print(\"Transformed features shape:\", X_transformed.shape)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Training and testing data prepared successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#define neural network \n",
    "class LapTimePredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LapTimePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model, loss function, and optimizer\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = LapTimePredictor(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#Prepare the data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop \n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        #forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        #backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert predictions and true values to numpy for easy plotting\n",
    "predicted = test_predictions.numpy().flatten()\n",
    "actual = y_test_tensor.numpy().flatten()\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actual, predicted, alpha=0.6)\n",
    "plt.plot([min(actual), max(actual)], [min(actual), max(actual)], color='red', linestyle='--')  # Diagonal line\n",
    "plt.xlabel('Actual Lap Times')\n",
    "plt.ylabel('Predicted Lap Times')\n",
    "plt.title('Actual vs. Predicted Lap Times')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     19\u001b[0m circuitId_bahrain \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Example circuit ID for Bahrain\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m average_lap_time_placeholder \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlapTime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Use the average lap time as a placeholder\u001b[39;00m\n\u001b[1;32m     22\u001b[0m predict_average_race_lap_times(circuitId_bahrain, average_lap_time_placeholder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_average_race_lap_times(circuit_id, average_lap_time):\n",
    "    # Get unique driver IDs from the dataset\n",
    "    driver_ids = merged_data['driverId'].unique()\n",
    "    \n",
    "    # List to store predicted lap times for each driver\n",
    "    predicted_lap_times = []\n",
    "\n",
    "    # Loop through each driver and predict lap time at the given circuit\n",
    "    for driver_id in driver_ids:\n",
    "        predicted_time = predict_lap_time(driver_id, circuit_id, average_lap_time)\n",
    "        predicted_lap_times.append(predicted_time)\n",
    "\n",
    "    # Calculate the average lap time for the race\n",
    "    average_lap_time = np.mean(predicted_lap_times)\n",
    "\n",
    "    print(f\"Predicted Average Lap Time for Circuit {circuit_id}: {average_lap_time:.2f} seconds\")\n",
    "\n",
    "# Example usage\n",
    "circuitId_bahrain = 3  # Example circuit ID for Bahrain\n",
    "average_lap_time_placeholder = merged_data['lapTime'].mean()  # Use the average lap time as a placeholder\n",
    "\n",
    "predict_average_race_lap_times(circuitId_bahrain, average_lap_time_placeholder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
