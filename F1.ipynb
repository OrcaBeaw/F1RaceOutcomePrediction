{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "%pip install requests pandas \n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'F1 dataset'\n",
      "/home/penguin/Documents/Code/F1RaceOutcomePrediction/F1 dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/Documents/Code/F1RaceOutcomePrediction/venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "#get data \n",
    "%cd F1 dataset \n",
    "circuits = pd.read_csv('circuits.csv')\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "lap_times = pd.read_csv('lap_times.csv')\n",
    "pit_stops = pd.read_csv('pit_stops.csv') \n",
    "qualifying = pd.read_csv('qualifying.csv')\n",
    "results = pd.read_csv('results.csv')\n",
    "seasons = pd.read_csv('seasons.csv')\n",
    "status = pd.read_csv('status.csv')\n",
    "\n",
    "#Explore the datasets\n",
    "datasets = {\n",
    "    \"Circuits\": circuits,\n",
    "    \"Drivers\": drivers,\n",
    "    \"Lap Times\": lap_times,\n",
    "    \"Pit Stops\": pit_stops,\n",
    "    \"Qualifying\": qualifying,\n",
    "    \"Results\": results,\n",
    "    \"Seasons\": seasons,\n",
    "    \"Status\": status\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "lap_times = datasets['Lap Times']\n",
    "drivers = datasets['Drivers']\n",
    "circuits = datasets['Circuits']\n",
    "\n",
    "\n",
    "#Data merging\n",
    "merged_data = lap_times.merge(drivers, on='driverId', how='inner')\n",
    "merged_data = merged_data.merge(circuits, left_on='driverId', right_on='circuitId', how='inner')\n",
    "\n",
    "#Convert lap times to seconds\n",
    "import pandas as pd\n",
    "\n",
    "def convert_to_seconds(time_str):\n",
    "    if isinstance(time_str, str):\n",
    "        minutes, seconds = time_str.split(':')\n",
    "        return int(minutes) * 60 + float(seconds)\n",
    "    return time_str\n",
    "\n",
    "# Ensure the correct column name is used\n",
    "if 'milliseconds' in merged_data.columns:\n",
    "    merged_data['lapTime'] = merged_data['milliseconds'].apply(lambda x: x / 1000)\n",
    "else:\n",
    "    raise KeyError(\"Column 'milliseconds' not found in the merged dataframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Ensure all lap times are converted to seconds before transforming\n",
    "merged_data['lapTime'] = merged_data['lapTime'].apply(convert_to_seconds)\n",
    "merged_data = merged_data.dropna(subset=['lapTime'])\n",
    "feature_columns = ['driverId', 'circuitId', 'lapTime']  \n",
    "\n",
    "# Separate features and target variable\n",
    "X = merged_data[feature_columns]\n",
    "y = merged_data['lapTime']  # If you're predicting future lap times, adjust accordingly\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['driverId', 'circuitId']\n",
    "numerical_features = ['lapTime']  # Include other numerical features if any\n",
    "\n",
    "# Transformer and scaler\n",
    "one_hot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot', one_hot, categorical_features),\n",
    "        ('scaler', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Exclude other columns not specified\n",
    ")\n",
    "\n",
    "# Apply the transformer to features\n",
    "X_transformed = transformer.fit_transform(X)\n",
    "print(\"Transformed features shape:\", X_transformed.shape)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Training and testing data prepared successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#define neural network \n",
    "class LapTimePredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LapTimePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model, loss function, and optimizer\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = LapTimePredictor(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#Prepare the data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop \n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        #forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        #backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
